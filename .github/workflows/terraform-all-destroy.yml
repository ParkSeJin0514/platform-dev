# ============================================================================
# .github/workflows/terraform-all-destroy.yml
# ============================================================================
# Terraform/Terragrunt ì¸í”„ë¼ ì‚­ì œ ì›Œí¬í”Œë¡œìš°
# - Karpenter ì™„ì „ ê°•ì œ ì‚­ì œ (Controller, NodeClaim, NodePool, EC2NodeClass, CRD)
# - Ingress Finalizer ì œê±° í›„ ì‚­ì œ (ALB ì •ë¦¬)
# - AWS CLIë¡œ ì§ì ‘ ë¦¬ì†ŒìŠ¤ ì‚­ì œ (EKS, RDS, EC2, NAT, ENI ë“±)
# - Terragrunt destroy ì‹¤í–‰
# ============================================================================

name: 'Terraform Destroy'

on:
  workflow_dispatch:
    inputs:
      confirm:
        description: 'ì‚­ì œ í™•ì¸ (destroy ìž…ë ¥)'
        required: true
        default: ''
      layer:
        description: 'ì‚­ì œ ë ˆì´ì–´ ì„ íƒ'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - bootstrap
          - compute
          - foundation

env:
  AWS_REGION: ap-northeast-2
  TF_VERSION: '1.9.0'
  TG_VERSION: '0.54.0'
  TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS: "true"
  PROJECT_NAME: "petclinic-kr"

permissions:
  id-token: write
  contents: read

jobs:
  # ============================================================================
  # í™•ì¸ ë‹¨ê³„
  # ============================================================================
  confirm:
    name: 'Confirm Destroy'
    runs-on: ubuntu-latest
    steps:
      - name: Check confirmation
        if: github.event.inputs.confirm != 'destroy'
        run: |
          echo "âŒ ì‚­ì œë¥¼ ì§„í–‰í•˜ë ¤ë©´ 'destroy'ë¥¼ ìž…ë ¥í•˜ì„¸ìš”."
          exit 1
      
      - name: Confirmed
        run: echo "âœ… ì‚­ì œê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤."

  # ============================================================================
  # EKS ë¦¬ì†ŒìŠ¤ ì‚¬ì „ ì •ë¦¬ (Kubernetes ë¦¬ì†ŒìŠ¤)
  # ============================================================================
  pre-cleanup:
    name: 'Pre-Cleanup EKS Resources'
    needs: confirm
    runs-on: ubuntu-latest
    outputs:
      vpc_id: ${{ steps.get-vpc.outputs.vpc_id }}
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install tools
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          sudo apt-get update && sudo apt-get install -y jq

      - name: Get VPC ID
        id: get-vpc
        run: |
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*${{ env.PROJECT_NAME }}*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
          echo "VPC ID: $VPC_ID"

      - name: Check EKS Cluster
        id: check-eks
        run: |
          CLUSTER_NAME=$(aws eks list-clusters --query 'clusters[0]' --output text 2>/dev/null || echo "")
          if [ -n "$CLUSTER_NAME" ] && [ "$CLUSTER_NAME" != "None" ]; then
            echo "cluster_exists=true" >> $GITHUB_OUTPUT
            echo "cluster_name=$CLUSTER_NAME" >> $GITHUB_OUTPUT
            echo "âœ… EKS í´ëŸ¬ìŠ¤í„° ë°œê²¬: $CLUSTER_NAME"
          else
            echo "cluster_exists=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ EKS í´ëŸ¬ìŠ¤í„° ì—†ìŒ - ì •ë¦¬ ìŠ¤í‚µ"
          fi

      - name: Update kubeconfig
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          aws eks update-kubeconfig --name ${{ steps.check-eks.outputs.cluster_name }} --region ${{ env.AWS_REGION }}

      # ========================================================================
      # 1ë‹¨ê³„: Karpenter Controller ì™„ì „ ì¤‘ì§€ (ìƒˆ ë…¸ë“œ ìƒì„± ë°©ì§€)
      # ========================================================================
      - name: Karpenter Step 1 - Stop Controller First
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== 1ë‹¨ê³„: Karpenter Controller ì™„ì „ ì¤‘ì§€ ==="
          echo "=============================================="
          
          # Deployment ìŠ¤ì¼€ì¼ ë‹¤ìš´ (ì¦‰ì‹œ)
          kubectl scale deploy karpenter -n kube-system --replicas=0 --timeout=30s 2>/dev/null || true
          
          # ëª¨ë“  Karpenter Pod ê°•ì œ ì‚­ì œ
          kubectl delete pods -l app.kubernetes.io/name=karpenter -n kube-system --force --grace-period=0 2>/dev/null || true
          
          # Deployment ìžì²´ ì‚­ì œ
          kubectl delete deploy karpenter -n kube-system --force --grace-period=0 2>/dev/null || true
          
          # ServiceAccount ì‚­ì œ (ìž¬ìƒì„± ë°©ì§€)
          kubectl delete sa karpenter -n kube-system --force --grace-period=0 2>/dev/null || true
          
          # Webhook ì‚­ì œ (NodeClaim ì‚­ì œ ì°¨ë‹¨ ë°©ì§€)
          kubectl delete validatingwebhookconfiguration karpenter-validation 2>/dev/null || true
          kubectl delete mutatingwebhookconfiguration karpenter-mutation 2>/dev/null || true
          
          echo "=== Controller ì™„ì „ ì¤‘ì§€ ëŒ€ê¸° (20ì´ˆ) ==="
          sleep 20
          
          # í™•ì¸
          echo "=== Karpenter Pod ìƒíƒœ ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=karpenter 2>/dev/null || echo "No Karpenter pods"
          
          echo "âœ… Karpenter Controller ì¤‘ì§€ ì™„ë£Œ"

      # ========================================================================
      # 2ë‹¨ê³„: Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ë¨¼ì € ì¢…ë£Œ (NodeClaim ì‚­ì œ ì „!)
      # ========================================================================
      - name: Karpenter Step 2 - Terminate EC2 Instances First
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== 2ë‹¨ê³„: Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ë¨¼ì € ì¢…ë£Œ ==="
          echo "=============================================="
          
          # ì—¬ëŸ¬ íƒœê·¸ íŒ¨í„´ìœ¼ë¡œ Karpenter ë…¸ë“œ ì°¾ê¸°
          echo "=== Karpenter ë…¸ë“œ ê²€ìƒ‰ ==="
          
          # íŒ¨í„´ 1: karpenter.sh/nodepool íƒœê·¸
          INSTANCES1=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" \
                      "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || true)
          
          # íŒ¨í„´ 2: karpenter.sh/managed-by íƒœê·¸
          INSTANCES2=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/managed-by" \
                      "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || true)
          
          # íŒ¨í„´ 3: karpenter.sh/nodeclaim íƒœê·¸
          INSTANCES3=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodeclaim" \
                      "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || true)
          
          # íŒ¨í„´ 4: node-type=karpenter íƒœê·¸
          INSTANCES4=$(aws ec2 describe-instances \
            --filters "Name=tag:node-type,Values=karpenter" \
                      "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || true)
          
          # ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
          ALL_INSTANCES="$INSTANCES1 $INSTANCES2 $INSTANCES3 $INSTANCES4"
          UNIQUE_INSTANCES=$(echo $ALL_INSTANCES | tr ' ' '\n' | grep -v '^$' | sort -u | tr '\n' ' ')
          
          if [ -n "$UNIQUE_INSTANCES" ] && [ "$UNIQUE_INSTANCES" != " " ]; then
            echo "ì¢…ë£Œí•  Karpenter ì¸ìŠ¤í„´ìŠ¤: $UNIQUE_INSTANCES"
            
            # ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ
            aws ec2 terminate-instances --instance-ids $UNIQUE_INSTANCES 2>/dev/null || true
            
            echo "=== EC2 ì¢…ë£Œ ëŒ€ê¸° (90ì´ˆ) ==="
            sleep 90
            
            # ì¢…ë£Œ í™•ì¸
            echo "=== ì¢…ë£Œ ìƒíƒœ í™•ì¸ ==="
            for INST in $UNIQUE_INSTANCES; do
              STATE=$(aws ec2 describe-instances --instance-ids $INST \
                --query 'Reservations[0].Instances[0].State.Name' --output text 2>/dev/null || echo "terminated")
              echo "  $INST: $STATE"
            done
          else
            echo "â„¹ï¸ Karpenter ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"
          fi
          
          echo "âœ… Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ ì™„ë£Œ"

      # ========================================================================
      # 3ë‹¨ê³„: NodeClaim ê°•ì œ ì‚­ì œ (EC2 ì¢…ë£Œ í›„)
      # ========================================================================
      - name: Karpenter Step 3 - Force Delete NodeClaims
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== 3ë‹¨ê³„: NodeClaim ê°•ì œ ì‚­ì œ ==="
          echo "=============================================="
          
          # ëª¨ë“  NodeClaim ì¡°íšŒ
          NODECLAIMS=$(kubectl get nodeclaims -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
          
          if [ -n "$NODECLAIMS" ]; then
            echo "ë°œê²¬ëœ NodeClaims: $NODECLAIMS"
            
            for NC in $NODECLAIMS; do
              echo "=== Processing NodeClaim: $NC ==="
              
              # ë°©ë²• 1: JSON Patchë¡œ Finalizer ì œê±°
              kubectl patch nodeclaim $NC --type=json \
                -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
              
              # ë°©ë²• 2: Merge Patchë¡œ Finalizer null ì„¤ì •
              kubectl patch nodeclaim $NC --type=merge \
                -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              
              # ë°©ë²• 3: Strategic Merge Patch
              kubectl patch nodeclaim $NC --type=strategic \
                -p '{"metadata":{"finalizers":[]}}' 2>/dev/null || true
              
              # ì‚­ì œ ì‹œë„
              kubectl delete nodeclaim $NC --force --grace-period=0 --timeout=30s 2>/dev/null &
            done
            
            # ë°±ê·¸ë¼ìš´ë“œ ì‚­ì œ ëŒ€ê¸°
            wait 2>/dev/null || true
            
            # ë‚¨ì€ NodeClaim í•œë²ˆ ë” ì •ë¦¬
            kubectl delete nodeclaims --all --force --grace-period=0 2>/dev/null || true
          else
            echo "â„¹ï¸ NodeClaim ì—†ìŒ"
          fi
          
          echo "=== NodeClaim ì‚­ì œ ëŒ€ê¸° (15ì´ˆ) ==="
          sleep 15
          
          # í™•ì¸
          echo "=== NodeClaim ìƒíƒœ ==="
          kubectl get nodeclaims 2>/dev/null || echo "No NodeClaims found"
          
          echo "âœ… NodeClaim ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # 4ë‹¨ê³„: NodePool ê°•ì œ ì‚­ì œ
      # ========================================================================
      - name: Karpenter Step 4 - Force Delete NodePools
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== 4ë‹¨ê³„: NodePool ê°•ì œ ì‚­ì œ ==="
          echo "=============================================="
          
          NODEPOOLS=$(kubectl get nodepools -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
          
          if [ -n "$NODEPOOLS" ]; then
            echo "ë°œê²¬ëœ NodePools: $NODEPOOLS"
            
            for NP in $NODEPOOLS; do
              echo "=== Processing NodePool: $NP ==="
              
              # Finalizer ì œê±°
              kubectl patch nodepool $NP --type=json \
                -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
              kubectl patch nodepool $NP --type=merge \
                -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              
              # ì‚­ì œ
              kubectl delete nodepool $NP --force --grace-period=0 2>/dev/null || true
            done
            
            kubectl delete nodepools --all --force --grace-period=0 2>/dev/null || true
          else
            echo "â„¹ï¸ NodePool ì—†ìŒ"
          fi
          
          echo "=== NodePool ì‚­ì œ ëŒ€ê¸° (10ì´ˆ) ==="
          sleep 10
          
          echo "=== NodePool ìƒíƒœ ==="
          kubectl get nodepools 2>/dev/null || echo "No NodePools found"
          
          echo "âœ… NodePool ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # 5ë‹¨ê³„: EC2NodeClass ê°•ì œ ì‚­ì œ
      # ========================================================================
      - name: Karpenter Step 5 - Force Delete EC2NodeClasses
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== 5ë‹¨ê³„: EC2NodeClass ê°•ì œ ì‚­ì œ ==="
          echo "=============================================="
          
          EC2NODECLASSES=$(kubectl get ec2nodeclasses -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
          
          if [ -n "$EC2NODECLASSES" ]; then
            echo "ë°œê²¬ëœ EC2NodeClasses: $EC2NODECLASSES"
            
            for EC2NC in $EC2NODECLASSES; do
              echo "=== Processing EC2NodeClass: $EC2NC ==="
              
              # Finalizer ì œê±°
              kubectl patch ec2nodeclass $EC2NC --type=json \
                -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
              kubectl patch ec2nodeclass $EC2NC --type=merge \
                -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              
              # ì‚­ì œ
              kubectl delete ec2nodeclass $EC2NC --force --grace-period=0 2>/dev/null || true
            done
            
            kubectl delete ec2nodeclasses --all --force --grace-period=0 2>/dev/null || true
          else
            echo "â„¹ï¸ EC2NodeClass ì—†ìŒ"
          fi
          
          echo "=== EC2NodeClass ì‚­ì œ ëŒ€ê¸° (10ì´ˆ) ==="
          sleep 10
          
          echo "=== EC2NodeClass ìƒíƒœ ==="
          kubectl get ec2nodeclasses 2>/dev/null || echo "No EC2NodeClasses found"
          
          echo "âœ… EC2NodeClass ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # 6ë‹¨ê³„: Karpenter CRD ê°•ì œ ì‚­ì œ
      # ========================================================================
      - name: Karpenter Step 6 - Force Delete CRDs
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== 6ë‹¨ê³„: Karpenter CRD ê°•ì œ ì‚­ì œ ==="
          echo "=============================================="
          
          KARPENTER_CRDS=$(kubectl get crd -o name 2>/dev/null | grep karpenter || true)
          
          if [ -n "$KARPENTER_CRDS" ]; then
            echo "ë°œê²¬ëœ Karpenter CRDs:"
            echo "$KARPENTER_CRDS"
            
            for CRD in $KARPENTER_CRDS; do
              CRD_NAME=$(echo $CRD | sed 's|customresourcedefinition.apiextensions.k8s.io/||')
              echo "=== Processing CRD: $CRD_NAME ==="
              
              # Finalizer ì œê±°
              kubectl patch crd $CRD_NAME --type=json \
                -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
              kubectl patch crd $CRD_NAME --type=merge \
                -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              
              # ë°±ê·¸ë¼ìš´ë“œ ì‚­ì œ
              kubectl delete crd $CRD_NAME --force --grace-period=0 &
            done
            
            # ë°±ê·¸ë¼ìš´ë“œ ì‚­ì œ ëŒ€ê¸°
            sleep 15
            wait 2>/dev/null || true
          else
            echo "â„¹ï¸ Karpenter CRD ì—†ìŒ"
          fi
          
          echo "=== Karpenter CRD ìƒíƒœ ==="
          kubectl get crd 2>/dev/null | grep karpenter || echo "No Karpenter CRDs found"
          
          echo "âœ… Karpenter CRD ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # 7ë‹¨ê³„: Karpenterê°€ ìƒì„±í•œ AWS ë¦¬ì†ŒìŠ¤ ì‚­ì œ
      # ========================================================================
      - name: Karpenter Step 7 - Delete AWS Resources (Instance Profiles & Launch Templates)
        run: |
          echo "=============================================="
          echo "=== 7ë‹¨ê³„: Karpenter AWS ë¦¬ì†ŒìŠ¤ ì‚­ì œ ==="
          echo "=============================================="
          
          # Karpenterê°€ ìƒì„±í•œ Instance Profile ì‚­ì œ
          echo "=== Instance Profile ì‚­ì œ ==="
          INSTANCE_PROFILES=$(aws iam list-instance-profiles \
            --query "InstanceProfiles[?contains(InstanceProfileName, '${{ env.PROJECT_NAME }}-eks_')].InstanceProfileName" \
            --output text 2>/dev/null || true)
          
          for IP in $INSTANCE_PROFILES; do
            echo "Processing Instance Profile: $IP"
            
            # Role ë¶„ë¦¬
            ROLES=$(aws iam get-instance-profile --instance-profile-name $IP \
              --query 'InstanceProfile.Roles[*].RoleName' --output text 2>/dev/null || true)
            for ROLE in $ROLES; do
              echo "  Removing role: $ROLE"
              aws iam remove-role-from-instance-profile \
                --instance-profile-name $IP --role-name $ROLE 2>/dev/null || true
            done
            
            # Instance Profile ì‚­ì œ
            aws iam delete-instance-profile --instance-profile-name $IP 2>/dev/null || true
            echo "  âœ“ Deleted: $IP"
          done
          
          # Karpenterê°€ ìƒì„±í•œ Launch Template ì‚­ì œ
          echo ""
          echo "=== Launch Template ì‚­ì œ ==="
          LAUNCH_TEMPLATES=$(aws ec2 describe-launch-templates \
            --query "LaunchTemplates[?contains(LaunchTemplateName, 'karpenter')].LaunchTemplateId" \
            --output text 2>/dev/null || true)
          
          for LT in $LAUNCH_TEMPLATES; do
            echo "Deleting Launch Template: $LT"
            aws ec2 delete-launch-template --launch-template-id $LT 2>/dev/null || true
          done
          
          # í”„ë¡œì íŠ¸ ì´ë¦„ìœ¼ë¡œë„ ê²€ìƒ‰
          LAUNCH_TEMPLATES2=$(aws ec2 describe-launch-templates \
            --query "LaunchTemplates[?contains(LaunchTemplateName, '${{ env.PROJECT_NAME }}')].LaunchTemplateId" \
            --output text 2>/dev/null || true)
          
          for LT in $LAUNCH_TEMPLATES2; do
            echo "Deleting Launch Template: $LT"
            aws ec2 delete-launch-template --launch-template-id $LT 2>/dev/null || true
          done
          
          echo "âœ… Karpenter AWS ë¦¬ì†ŒìŠ¤ ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # Karpenter ì‚­ì œ ìµœì¢… í™•ì¸
      # ========================================================================
      - name: Karpenter Cleanup Summary
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== Karpenter ì •ë¦¬ ìµœì¢… í™•ì¸ ==="
          echo "=============================================="
          echo ""
          echo "ðŸ“‹ Kubernetes ë¦¬ì†ŒìŠ¤:"
          echo "  NodeClaims:"
          kubectl get nodeclaims 2>/dev/null || echo "    None"
          echo "  NodePools:"
          kubectl get nodepools 2>/dev/null || echo "    None"
          echo "  EC2NodeClasses:"
          kubectl get ec2nodeclasses 2>/dev/null || echo "    None"
          echo "  Karpenter CRDs:"
          kubectl get crd 2>/dev/null | grep karpenter || echo "    None"
          echo "  Karpenter Pods:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=karpenter 2>/dev/null || echo "    None"
          echo ""
          echo "ðŸ“‹ AWS ë¦¬ì†ŒìŠ¤:"
          echo "  Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤:"
          aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" \
                      "Name=instance-state-name,Values=running,pending" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || echo "    None"
          echo ""
          echo "âœ… Karpenter ê°•ì œ ì‚­ì œ ì™„ë£Œ!"

      # ========================================================================
      # ArgoCD ì •ë¦¬
      # ========================================================================
      - name: Stop ArgoCD Components
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== ArgoCD ì „ì²´ ì»´í¬ë„ŒíŠ¸ ì¤‘ì§€ ==="
          kubectl scale deploy --all -n argocd --replicas=0 2>/dev/null || true
          kubectl scale statefulset --all -n argocd --replicas=0 2>/dev/null || true
          kubectl delete deploy --all -n argocd --force --grace-period=0 2>/dev/null || true
          sleep 10
          kubectl delete pods --all -n argocd --force --grace-period=0 2>/dev/null || true
          echo "âœ… ArgoCD ì»´í¬ë„ŒíŠ¸ ì¤‘ì§€ ì™„ë£Œ"

      - name: Cleanup ArgoCD Applications
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== ArgoCD Application Finalizer ì œê±° ==="
          for app in $(kubectl get applications -n argocd -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
            kubectl patch application $app -n argocd --type=json \
              -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
            kubectl patch application $app -n argocd --type=merge \
              -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
          done
          kubectl delete applications.argoproj.io --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          for appset in $(kubectl get applicationsets -n argocd -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
            kubectl patch applicationset $appset -n argocd --type=merge \
              -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
          done
          kubectl delete applicationsets.argoproj.io --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          for proj in $(kubectl get appprojects -n argocd -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
            kubectl patch appproject $proj -n argocd --type=merge \
              -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
          done
          kubectl delete appprojects.argoproj.io --all -n argocd --force --grace-period=0 2>/dev/null || true
          
          # ArgoCD CRD ì‚­ì œ
          for CRD in applications.argoproj.io applicationsets.argoproj.io appprojects.argoproj.io; do
            if kubectl get crd $CRD > /dev/null 2>&1; then
              kubectl patch crd $CRD --type=merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              kubectl delete crd $CRD --force --grace-period=0 &
            fi
          done
          sleep 5
          echo "âœ… ArgoCD ì •ë¦¬ ì™„ë£Œ"

      # ========================================================================
      # External Secrets ì •ë¦¬
      # ========================================================================
      - name: Cleanup External Secrets
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== External Secrets ì •ë¦¬ ==="
          kubectl scale deploy --all -n external-secrets --replicas=0 2>/dev/null || true
          kubectl delete deploy --all -n external-secrets --force --grace-period=0 2>/dev/null || true
          kubectl delete pods --all -n external-secrets --force --grace-period=0 2>/dev/null || true
          
          for css in $(kubectl get clustersecretstore -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
            kubectl patch clustersecretstore $css --type=merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
          done
          kubectl delete clustersecretstore --all --force --grace-period=0 2>/dev/null || true
          kubectl delete externalsecret --all -A --force --grace-period=0 2>/dev/null || true
          
          for CRD in externalsecrets.external-secrets.io clustersecretstores.external-secrets.io secretstores.external-secrets.io; do
            if kubectl get crd $CRD > /dev/null 2>&1; then
              kubectl patch crd $CRD --type=merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              kubectl delete crd $CRD --force --grace-period=0 &
            fi
          done
          sleep 5
          echo "âœ… External Secrets ì •ë¦¬ ì™„ë£Œ"

      # ========================================================================
      # Ingress Finalizer ì œê±° ë° ì‚­ì œ (â˜… ìˆ˜ì •ë¨)
      # ========================================================================
      - name: Delete All Ingresses with Finalizer Removal
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=============================================="
          echo "=== Ingress Finalizer ì œê±° ë° ì‚­ì œ ==="
          echo "=============================================="
          
          # ëª¨ë“  ë„¤ìž„ìŠ¤íŽ˜ì´ìŠ¤ì˜ Ingress ì¡°íšŒ ë° Finalizer ì œê±°
          echo "=== 1. Ingress Finalizer ì œê±° ==="
          for NS in $(kubectl get ns -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
            INGRESSES=$(kubectl get ingress -n $NS -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || true)
            for ING in $INGRESSES; do
              if [ -n "$ING" ]; then
                echo "Processing ingress/$ING in namespace $NS"
                
                # Finalizer ì œê±° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
                kubectl patch ingress $ING -n $NS --type=merge \
                  -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
                kubectl patch ingress $ING -n $NS --type=json \
                  -p='[{"op": "remove", "path": "/metadata/finalizers"}]' 2>/dev/null || true
              fi
            done
          done
          
          # Ingress ê°•ì œ ì‚­ì œ
          echo ""
          echo "=== 2. Ingress ê°•ì œ ì‚­ì œ ==="
          kubectl delete ingress --all -A --force --grace-period=0 2>/dev/null || true
          
          # ì‚­ì œ ëŒ€ê¸°
          echo "=== ì‚­ì œ ëŒ€ê¸° (15ì´ˆ) ==="
          sleep 15
          
          # ë‚¨ì€ Ingress í™•ì¸ ë° ìž¬ì‹œë„
          echo ""
          echo "=== 3. ë‚¨ì€ Ingress ìž¬í™•ì¸ ë° ìž¬ì‹œë„ ==="
          REMAINING_INGRESSES=$(kubectl get ingress -A --no-headers 2>/dev/null || true)
          if [ -n "$REMAINING_INGRESSES" ]; then
            echo "âš ï¸ ë‚¨ì€ Ingress ë°œê²¬:"
            echo "$REMAINING_INGRESSES"
            echo ""
            echo "ìž¬ì‹œë„ ì¤‘..."
            
            # ê°œë³„ ì²˜ë¦¬
            kubectl get ingress -A -o jsonpath='{range .items[*]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' 2>/dev/null | \
            while read NS ING; do
              if [ -n "$NS" ] && [ -n "$ING" ]; then
                echo "  Force deleting: $NS/$ING"
                kubectl patch ingress $ING -n $NS --type=merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
                kubectl delete ingress $ING -n $NS --force --grace-period=0 --timeout=30s 2>/dev/null || true
              fi
            done
            
            sleep 10
          fi
          
          # ìµœì¢… ìƒíƒœ í™•ì¸
          echo ""
          echo "=== Ingress ìµœì¢… ìƒíƒœ ==="
          kubectl get ingress -A 2>/dev/null || echo "No ingresses found âœ…"
          
          echo ""
          echo "âœ… Ingress ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # ALB ë° Target Group ì‚­ì œ
      # ========================================================================
      - name: Delete ALBs and Target Groups
        run: |
          echo "=============================================="
          echo "=== ALB ì‚­ì œ ==="
          echo "=============================================="
          
          # k8s- ë˜ëŠ” í”„ë¡œì íŠ¸ëª… í¬í•¨ ALB ì‚­ì œ
          aws elbv2 describe-load-balancers --query 'LoadBalancers[*].[LoadBalancerArn,LoadBalancerName]' --output text 2>/dev/null | \
          while IFS=$'\t' read -r ARN NAME; do
            if [ -n "$ARN" ] && [ "$ARN" != "None" ]; then
              if [[ "$NAME" == *petclinic* ]] || [[ "$NAME" == *argocd* ]] || [[ "$NAME" == k8s-* ]]; then
                echo "Deleting ALB: $NAME ($ARN)"
                
                # ë¦¬ìŠ¤ë„ˆ ë¨¼ì € ì‚­ì œ
                LISTENERS=$(aws elbv2 describe-listeners --load-balancer-arn "$ARN" --query 'Listeners[*].ListenerArn' --output text 2>/dev/null || true)
                for L in $LISTENERS; do
                  if [ -n "$L" ] && [ "$L" != "None" ]; then
                    echo "  Deleting listener: $L"
                    aws elbv2 delete-listener --listener-arn "$L" 2>/dev/null || true
                  fi
                done
                
                # ALB ì‚­ì œ
                aws elbv2 delete-load-balancer --load-balancer-arn "$ARN" 2>/dev/null || true
                echo "  âœ“ ALB ì‚­ì œ ìš”ì²­ ì™„ë£Œ"
              fi
            fi
          done
          
          echo ""
          echo "=== ALB ì‚­ì œ ëŒ€ê¸° (60ì´ˆ) ==="
          sleep 60
          
          echo ""
          echo "=== Target Group ì‚­ì œ ==="
          aws elbv2 describe-target-groups --query 'TargetGroups[*].[TargetGroupArn,TargetGroupName]' --output text 2>/dev/null | \
          while IFS=$'\t' read -r ARN NAME; do
            if [ -n "$ARN" ] && [ "$ARN" != "None" ]; then
              if [[ "$NAME" == *petclinic* ]] || [[ "$NAME" == *argocd* ]] || [[ "$NAME" == k8s-* ]]; then
                echo "Deleting Target Group: $NAME"
                aws elbv2 delete-target-group --target-group-arn "$ARN" 2>/dev/null || true
              fi
            fi
          done
          
          echo ""
          echo "âœ… ALB ë° Target Group ì‚­ì œ ì™„ë£Œ"

      # ========================================================================
      # Namespace ì •ë¦¬
      # ========================================================================
      - name: Cleanup Namespaces
        if: steps.check-eks.outputs.cluster_exists == 'true'
        run: |
          echo "=== Namespace ì •ë¦¬ ==="
          for NS in petclinic monitoring argocd external-secrets; do
            if kubectl get namespace $NS > /dev/null 2>&1; then
              echo "Processing namespace: $NS"
              kubectl delete all --all -n $NS --force --grace-period=0 2>/dev/null || true
              kubectl patch namespace $NS --type=merge -p '{"metadata":{"finalizers":null}}' 2>/dev/null || true
              kubectl delete namespace $NS --force --grace-period=0 2>/dev/null || true
              
              # ê°•ì œ ì‚­ì œ (API ì§ì ‘ í˜¸ì¶œ)
              kubectl get namespace $NS -o json 2>/dev/null | \
                jq '.spec.finalizers = []' | \
                kubectl replace --raw "/api/v1/namespaces/$NS/finalize" -f - 2>/dev/null || true
            fi
          done
          
          echo "âœ… Namespace ì •ë¦¬ ì™„ë£Œ"

      - name: Pre-Cleanup Summary
        run: |
          echo "=============================================="
          echo "âœ… Pre-cleanup ì™„ë£Œ!"
          echo "=============================================="

  # ============================================================================
  # AWS ë¦¬ì†ŒìŠ¤ ê°•ì œ ì‚­ì œ
  # ============================================================================
  aws-cleanup:
    name: 'AWS Resources Cleanup'
    needs: pre-cleanup
    runs-on: ubuntu-latest
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Get VPC ID
        id: get-vpc
        run: |
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*${{ env.PROJECT_NAME }}*" \
            --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "None")
          echo "vpc_id=$VPC_ID" >> $GITHUB_OUTPUT
          echo "VPC ID: $VPC_ID"

      # ========================================================================
      # Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ í•œë²ˆ ë” í™•ì¸ ë° ì¢…ë£Œ
      # ========================================================================
      - name: Final Karpenter EC2 Cleanup
        run: |
          echo "=== Karpenter EC2 ì¸ìŠ¤í„´ìŠ¤ ìµœì¢… ì •ë¦¬ ==="
          
          KARPENTER_INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag-key,Values=karpenter.sh/nodepool" \
                      "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' \
            --output text 2>/dev/null || true)
          
          if [ -n "$KARPENTER_INSTANCES" ] && [ "$KARPENTER_INSTANCES" != "None" ]; then
            echo "Terminating remaining Karpenter instances: $KARPENTER_INSTANCES"
            aws ec2 terminate-instances --instance-ids $KARPENTER_INSTANCES 2>/dev/null || true
            sleep 30
          else
            echo "â„¹ï¸ ë‚¨ì€ Karpenter ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"
          fi

      # ========================================================================
      # Karpenter Instance Profile ìµœì¢… ì •ë¦¬
      # ========================================================================
      - name: Final Karpenter Instance Profile Cleanup
        run: |
          echo "=== Karpenter Instance Profile ìµœì¢… ì •ë¦¬ ==="
          
          # í”„ë¡œì íŠ¸ëª…_ìˆ«ìž íŒ¨í„´ì˜ Instance Profile (Karpenter ìžë™ ìƒì„±)
          INSTANCE_PROFILES=$(aws iam list-instance-profiles \
            --query 'InstanceProfiles[*].InstanceProfileName' --output text 2>/dev/null || true)
          
          for IP in $INSTANCE_PROFILES; do
            if [[ "$IP" == "${{ env.PROJECT_NAME }}-eks_"* ]] || [[ "$IP" == *"karpenter"* ]]; then
              echo "Processing: $IP"
              
              # Role ë¶„ë¦¬
              ROLES=$(aws iam get-instance-profile --instance-profile-name "$IP" \
                --query 'InstanceProfile.Roles[*].RoleName' --output text 2>/dev/null || true)
              for ROLE in $ROLES; do
                aws iam remove-role-from-instance-profile \
                  --instance-profile-name "$IP" --role-name "$ROLE" 2>/dev/null || true
              done
              
              # Instance Profile ì‚­ì œ
              aws iam delete-instance-profile --instance-profile-name "$IP" 2>/dev/null || true
              echo "  âœ“ Deleted: $IP"
            fi
          done
          
          echo "âœ… Instance Profile ì •ë¦¬ ì™„ë£Œ"

      # ========================================================================
      # EKS Node Group ì‚­ì œ
      # ========================================================================
      - name: Delete EKS Node Groups
        run: |
          echo "=== EKS Node Group ì‚­ì œ ==="
          CLUSTER_NAME="${{ env.PROJECT_NAME }}-eks"
          
          NODE_GROUPS=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --query 'nodegroups[*]' --output text 2>/dev/null || true)
          
          for NG in $NODE_GROUPS; do
            echo "Deleting Node Group: $NG"
            aws eks delete-nodegroup --cluster-name $CLUSTER_NAME --nodegroup-name $NG 2>/dev/null || true
          done
          
          if [ -n "$NODE_GROUPS" ] && [ "$NODE_GROUPS" != "None" ]; then
            echo "=== Node Group ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 10ë¶„) ==="
            for i in {1..20}; do
              REMAINING=$(aws eks list-nodegroups --cluster-name $CLUSTER_NAME --query 'nodegroups' --output text 2>/dev/null || echo "")
              if [ -z "$REMAINING" ] || [ "$REMAINING" == "None" ]; then
                echo "âœ… ëª¨ë“  Node Group ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "ëŒ€ê¸° ì¤‘... ($i/20)"
              sleep 30
            done
          fi

      # ========================================================================
      # EKS Cluster ì‚­ì œ
      # ========================================================================
      - name: Delete EKS Cluster
        run: |
          echo "=== EKS Cluster ì‚­ì œ ==="
          CLUSTER_NAME="${{ env.PROJECT_NAME }}-eks"
          
          if aws eks describe-cluster --name $CLUSTER_NAME > /dev/null 2>&1; then
            echo "Deleting EKS Cluster: $CLUSTER_NAME"
            aws eks delete-cluster --name $CLUSTER_NAME 2>/dev/null || true
            
            echo "=== EKS Cluster ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 10ë¶„) ==="
            for i in {1..20}; do
              if ! aws eks describe-cluster --name $CLUSTER_NAME > /dev/null 2>&1; then
                echo "âœ… EKS Cluster ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "ëŒ€ê¸° ì¤‘... ($i/20)"
              sleep 30
            done
          else
            echo "â„¹ï¸ EKS Cluster ì—†ìŒ"
          fi

      # ========================================================================
      # RDS ì‚­ì œ
      # ========================================================================
      - name: Delete RDS Instance
        run: |
          echo "=== RDS ì‚­ì œ ==="
          DB_INSTANCE="${{ env.PROJECT_NAME }}-db"
          
          if aws rds describe-db-instances --db-instance-identifier $DB_INSTANCE > /dev/null 2>&1; then
            echo "Deleting RDS: $DB_INSTANCE"
            aws rds delete-db-instance \
              --db-instance-identifier $DB_INSTANCE \
              --skip-final-snapshot \
              --delete-automated-backups 2>/dev/null || true
            
            echo "=== RDS ì‚­ì œ ëŒ€ê¸° (ìµœëŒ€ 10ë¶„) ==="
            for i in {1..20}; do
              if ! aws rds describe-db-instances --db-instance-identifier $DB_INSTANCE > /dev/null 2>&1; then
                echo "âœ… RDS ì‚­ì œ ì™„ë£Œ"
                break
              fi
              echo "ëŒ€ê¸° ì¤‘... ($i/20)"
              sleep 30
            done
          else
            echo "â„¹ï¸ RDS ì—†ìŒ"
          fi

      # ========================================================================
      # EC2 ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ
      # ========================================================================
      - name: Delete EC2 Instances
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== EC2 ì¸ìŠ¤í„´ìŠ¤ ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=instance-state-name,Values=running,pending,stopping,stopped" \
            --query 'Reservations[*].Instances[*].InstanceId' --output text 2>/dev/null || true)
          
          if [ -n "$INSTANCES" ] && [ "$INSTANCES" != "None" ]; then
            echo "Terminating instances: $INSTANCES"
            aws ec2 terminate-instances --instance-ids $INSTANCES 2>/dev/null || true
            
            echo "=== EC2 ì¢…ë£Œ ëŒ€ê¸° (90ì´ˆ) ==="
            sleep 90
          else
            echo "â„¹ï¸ EC2 ì¸ìŠ¤í„´ìŠ¤ ì—†ìŒ"
          fi

      # ========================================================================
      # NAT Gateway ì‚­ì œ
      # ========================================================================
      - name: Delete NAT Gateways
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== NAT Gateway ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          NAT_IDS=$(aws ec2 describe-nat-gateways \
            --filter "Name=vpc-id,Values=$VPC_ID" "Name=state,Values=available,pending" \
            --query 'NatGateways[*].NatGatewayId' --output text 2>/dev/null || true)
          
          for NAT in $NAT_IDS; do
            echo "Deleting NAT Gateway: $NAT"
            aws ec2 delete-nat-gateway --nat-gateway-id $NAT 2>/dev/null || true
          done
          
          if [ -n "$NAT_IDS" ] && [ "$NAT_IDS" != "None" ]; then
            echo "=== NAT Gateway ì‚­ì œ ëŒ€ê¸° (120ì´ˆ) ==="
            sleep 120
          fi

      # ========================================================================
      # Elastic IP í•´ì œ
      # ========================================================================
      - name: Release Elastic IPs
        run: |
          echo "=== Elastic IP í•´ì œ ==="
          EIPS=$(aws ec2 describe-addresses \
            --query 'Addresses[?AssociationId==`null`].AllocationId' --output text 2>/dev/null || true)
          
          for EIP in $EIPS; do
            echo "Releasing EIP: $EIP"
            aws ec2 release-address --allocation-id $EIP 2>/dev/null || true
          done

      # ========================================================================
      # ENI ì‚­ì œ
      # ========================================================================
      - name: Delete Network Interfaces
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== ENI ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          ENIS=$(aws ec2 describe-network-interfaces \
            --filters "Name=vpc-id,Values=$VPC_ID" "Name=status,Values=available" \
            --query 'NetworkInterfaces[*].NetworkInterfaceId' --output text 2>/dev/null || true)
          
          for ENI in $ENIS; do
            echo "Deleting ENI: $ENI"
            aws ec2 delete-network-interface --network-interface-id $ENI 2>/dev/null || true
          done

      # ========================================================================
      # Security Group ì‚­ì œ
      # ========================================================================
      - name: Delete Security Groups
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== Security Group ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          SGS=$(aws ec2 describe-security-groups \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text 2>/dev/null || true)
          
          # ê·œì¹™ ë¨¼ì € ì œê±°
          for SG in $SGS; do
            aws ec2 describe-security-groups --group-ids $SG --query 'SecurityGroups[0].IpPermissions' --output json 2>/dev/null | \
              jq -c '.[]?' | while read -r rule; do
                aws ec2 revoke-security-group-ingress --group-id $SG --ip-permissions "$rule" 2>/dev/null || true
              done
            aws ec2 describe-security-groups --group-ids $SG --query 'SecurityGroups[0].IpPermissionsEgress' --output json 2>/dev/null | \
              jq -c '.[]?' | while read -r rule; do
                aws ec2 revoke-security-group-egress --group-id $SG --ip-permissions "$rule" 2>/dev/null || true
              done
          done
          
          # SG ì‚­ì œ
          for SG in $SGS; do
            echo "Deleting SG: $SG"
            aws ec2 delete-security-group --group-id $SG 2>/dev/null || true
          done

      # ========================================================================
      # RDS Subnet Group ì‚­ì œ
      # ========================================================================
      - name: Delete RDS Subnet Groups
        run: |
          echo "=== RDS Subnet Group ì‚­ì œ ==="
          SUBNET_GROUPS=$(aws rds describe-db-subnet-groups \
            --query 'DBSubnetGroups[?contains(DBSubnetGroupName, `petclinic`)].DBSubnetGroupName' \
            --output text 2>/dev/null || true)
          
          for SG in $SUBNET_GROUPS; do
            echo "Deleting RDS Subnet Group: $SG"
            aws rds delete-db-subnet-group --db-subnet-group-name $SG 2>/dev/null || true
          done

      # ========================================================================
      # Route Table ì‚­ì œ
      # ========================================================================
      - name: Delete Route Tables
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== Route Table ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          RTS=$(aws ec2 describe-route-tables \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'RouteTables[?Associations[0].Main!=`true`].RouteTableId' --output text 2>/dev/null || true)
          
          for RT in $RTS; do
            ASSOCS=$(aws ec2 describe-route-tables --route-table-ids $RT \
              --query 'RouteTables[0].Associations[?!Main].RouteTableAssociationId' --output text 2>/dev/null || true)
            for ASSOC in $ASSOCS; do
              aws ec2 disassociate-route-table --association-id $ASSOC 2>/dev/null || true
            done
            echo "Deleting Route Table: $RT"
            aws ec2 delete-route-table --route-table-id $RT 2>/dev/null || true
          done

      # ========================================================================
      # Subnet ì‚­ì œ
      # ========================================================================
      - name: Delete Subnets
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== Subnet ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          SUBNETS=$(aws ec2 describe-subnets \
            --filters "Name=vpc-id,Values=$VPC_ID" \
            --query 'Subnets[*].SubnetId' --output text 2>/dev/null || true)
          
          for SUBNET in $SUBNETS; do
            echo "Deleting Subnet: $SUBNET"
            aws ec2 delete-subnet --subnet-id $SUBNET 2>/dev/null || true
          done

      # ========================================================================
      # Internet Gateway ì‚­ì œ
      # ========================================================================
      - name: Delete Internet Gateway
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== Internet Gateway ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          IGW=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
            --query 'InternetGateways[0].InternetGatewayId' --output text 2>/dev/null || true)
          
          if [ -n "$IGW" ] && [ "$IGW" != "None" ]; then
            echo "Detaching IGW: $IGW"
            aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_ID 2>/dev/null || true
            echo "Deleting IGW: $IGW"
            aws ec2 delete-internet-gateway --internet-gateway-id $IGW 2>/dev/null || true
          fi

      # ========================================================================
      # VPC ì‚­ì œ
      # ========================================================================
      - name: Delete VPC
        if: steps.get-vpc.outputs.vpc_id != 'None'
        run: |
          echo "=== VPC ì‚­ì œ ==="
          VPC_ID="${{ steps.get-vpc.outputs.vpc_id }}"
          
          echo "Deleting VPC: $VPC_ID"
          aws ec2 delete-vpc --vpc-id $VPC_ID 2>/dev/null || true

      - name: AWS Cleanup Summary
        run: |
          echo "âœ… AWS ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ!"

  # ============================================================================
  # Terraform State ì •ë¦¬
  # ============================================================================
  destroy:
    name: 'Terraform State Cleanup'
    needs: aws-cleanup
    runs-on: ubuntu-latest
    env:
      TERRAGRUNT_IGNORE_DEPENDENCY_ERRORS: "true"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false

      - name: Setup Terragrunt
        run: |
          wget -q https://github.com/gruntwork-io/terragrunt/releases/download/v${{ env.TG_VERSION }}/terragrunt_linux_amd64
          chmod +x terragrunt_linux_amd64
          sudo mv terragrunt_linux_amd64 /usr/local/bin/terragrunt

      - name: Create SSH Key
        run: |
          mkdir -p keys
          if [ -n "${{ secrets.SSH_PUBLIC_KEY }}" ]; then
            echo "${{ secrets.SSH_PUBLIC_KEY }}" > keys/test.pub
          else
            ssh-keygen -t rsa -b 4096 -f keys/test -N "" -q
          fi

      - name: Destroy Bootstrap
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'bootstrap'
        working-directory: ./bootstrap
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "=== Bootstrap Layer State ì •ë¦¬ ==="
          terragrunt destroy -auto-approve --terragrunt-non-interactive 2>/dev/null || true

      - name: Destroy Compute
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'compute'
        working-directory: ./compute
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "=== Compute Layer State ì •ë¦¬ ==="
          terragrunt destroy -auto-approve --terragrunt-non-interactive 2>/dev/null || true

      - name: Destroy Foundation
        if: github.event.inputs.layer == 'all' || github.event.inputs.layer == 'foundation'
        working-directory: ./foundation
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_db_password }}
        run: |
          echo "=== Foundation Layer State ì •ë¦¬ ==="
          terragrunt destroy -auto-approve --terragrunt-non-interactive 2>/dev/null || true

      - name: Verify Destruction
        run: |
          echo "=============================================="
          echo "=== ì‚­ì œ ê²€ì¦ ==="
          echo "=============================================="
          echo ""
          echo "EKS í´ëŸ¬ìŠ¤í„°:"
          aws eks list-clusters --query 'clusters' --output text || echo "None"
          echo ""
          echo "RDS ì¸ìŠ¤í„´ìŠ¤:"
          aws rds describe-db-instances --query 'DBInstances[*].DBInstanceIdentifier' --output text || echo "None"
          echo ""
          echo "VPC (petclinic ê´€ë ¨):"
          aws ec2 describe-vpcs --filters "Name=tag:Name,Values=*petclinic*" --query 'Vpcs[*].VpcId' --output text || echo "None"
          echo ""
          echo "EC2 ì¸ìŠ¤í„´ìŠ¤ (petclinic ê´€ë ¨):"
          aws ec2 describe-instances --filters "Name=tag:Name,Values=*petclinic*" "Name=instance-state-name,Values=running" --query 'Reservations[*].Instances[*].InstanceId' --output text || echo "None"
          echo ""
          echo "ALB:"
          aws elbv2 describe-load-balancers --query 'LoadBalancers[?contains(LoadBalancerName, `k8s-`) || contains(LoadBalancerName, `petclinic`)].LoadBalancerName' --output text || echo "None"

      - name: Destroy Complete
        run: |
          echo "=============================================="
          echo "ðŸŽ‰ Terraform Destroy ì™„ë£Œ!"
          echo "=============================================="
          echo ""
          echo "ì‚­ì œëœ ë ˆì´ì–´: ${{ github.event.inputs.layer }}"